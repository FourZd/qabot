[
    {
        "title": "Zero-Shot Prompting",
        "content": "\n            Большие языковые модели (LLMs), такие как GPT-3, настроены на следование инструкциям и обучаются на больших объемах данных, поэтому они способны выполнять некоторые задачи \"нулевой разметки\".\n            Мы протестировали несколько примеров нулевой разметки в предыдущем разделе. Вот один из примеров, которые мы использовали:\n            Запрос:\n            Classify the text into neutral, negative or positive. \n            Text: I think the vacation is okay.\n            Sentiment:\n            Результат:\n            Neutral\n            Обратите внимание, что в данном запросе мы не предоставили модели никаких примеров текста с их классификациями, однако LLM уже понимает \"настроение\" - это возможности нулевой разметки в действии.\n            Тюнинг инструкций показал улучшение в нулевом обучении Wei et al. (2022). Тюнинг инструкций (Instruction tuning) представляет собой концепцию донастройки моделей на наборах данных, описанных с помощью инструкций. Более того, был применен RLHF (усиление обучения на основе обратной связи от человека) RLHF для масштабирования тюнинга инструкций, при котором модель настраивается на лучшее соответствие предпочтениям людей. Это недавнее развитие позволяет моделям, таким как ChatGPT, проявлять такие возможности. Мы рассмотрим все эти подходы и методы в следующих разделах.\n            Когда Zero-Shot промптинг не работает, рекомендуется предоставить демонстрации или примеры в запросе, что приводит к few-shot промптингу. В следующем разделе мы это продемонстрируем.\n        "
    },
    {
        "title": "Few-Shot Prompting",
        "content": "\n        Большие языковые модели продемонстрировали впечатляющие возможности zero-shot промптинга, однако они все еще ограничены в более сложных задачах при использовании zero-shot настроек. Few-shot промптинг может использоваться в качестве техники для обеспечения контекстного обучения, когда мы предоставляем демонстрации в запросе, чтобы направить модель на более высокую производительность. Демонстрации служат в качестве контекста для последующих примеров, в которых мы хотим, чтобы модель генерировала ответ.\n        "
    },
    {
        "title": "Chain-of-Thought Prompting",
        "content": "\n            Введенная в Wei et al. (2022) техника формулировки промптов \"цепочка мыслей\" (CoT) позволяет выполнять сложные рассуждения с помощью промежуточных шагов рассуждения. Вы можете комбинировать ее с few-shot, чтобы получить лучшие результаты в более сложных задачах, требующих рассуждения перед ответом.\n            Запрос:\n            The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n            A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n            The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n            A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n            The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n            A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n            The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n            A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n            The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n            A:\n            Результат:\n            Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n            Ого! Мы видим отличный результат, когда предоставляем шаг рассуждения. Фактически, мы можем решить эту задачу, предоставив еще меньше примеров. Одного примера, кажется,достаточно:\n            Запрос:\n            The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n            A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n            The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n            A:\n            Результат:\n            Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n            Имейте в виду, что авторы утверждают, что это возникающая способность, которая проявляется у достаточно больших языковых моделях.\n        "
    },
    {
        "title": "Self-Consistency",
        "content": "\n        Одним из более продвинутых методов оптимизации запросов является метод самосогласованности. Предложенный в Wang et al. (2022), метод самосогласованности стремится \"заменить простую жадную декодировку, используемую в методе few-shot CoT\". Идея заключается в выборе нескольких различных путей рассуждения через метод few-shot CoT, а затем использовании генераций для выбора наиболее согласованного ответа. Это помогает улучшить производительность CoT-метода в задачах, связанных с арифметикой и здравым смыслом.\n        "
    },
    {
        "title": "Generate Knowledge Prompting",
        "content": "\n            Большие языковые модели постоянно совершенствуются, и одной из популярных техник является возможность интегрировать знания или информацию, чтобы помочь модели делать более точные прогнозы.\n            Используя подобную идею, можно ли также использовать модель для генерации знаний перед предсказанием? Именно это пытается сделать статья Liu et al. 2022 - генерировать знания для использования в качестве части запроса. В частности, насколько полезно это для задач, связанных с здравым смыслом?\n            Давайте попробуем простой запрос:\n            Запрос:\n            Part of golf is trying to get a higher point total than others. Yes or No?\n            Результат:\n            Yes.\n            Такая ошибка выявляет ограничения LLM в выполнении задач, требующих большего количества знаний о мире. Как можно улучшить это с помощью генерации знаний?\n            Сначала сгенерируем несколько \"знаний\":\n            Запрос:\n            Input: Greece is larger than mexico.\n            Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n            Input: Glasses always fog up.\n            Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n            Input: A fish is capable of thinking.\n            Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\n            Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n            Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n            Input: A rock is the same size as a pebble.\n            Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n            Input: Part of golf is trying to get a higher point total than others.\n            Knowledge:\n            Знание 1:\n            The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n            Знание 2:\n            Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n            Мы используем запрос, представленный в статье Liu et al. 2022.\n            Следующий шаг - интеграция знаний и получение предсказания. Я переформулировал вопрос в формате вопрос-ответ, чтобы указать формат ответа.\n            Запрос:\n            Question: Part of golf is trying to get a higher point total than others. Yes or No?\n            Knowledge: The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n            Explain and Answer: \n            Ответ 1 (очень высокая уверенность):\n            No, the objective of golf is not to get a higher point total than others. Rather, the objective is to play a set of holes in the least number of strokes. The total number of strokes is used to determine the winner of the game, not the total number of points.\n            Question: Part of golf is trying to get a higher point total than others. Yes or No?\n            Knowledge: Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n            Explain and Answer:\n            Ответ 2 (уверенность намного ниже):\n            Yes, part of golf is trying to get a higher point total than others. Each player tries to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n            С этим примером происходят действительно интересные вещи. В первом ответе модель была очень уверена, но во втором ответе уже не так уверена. Я упростил процесс для целей демонстрации, но при определении окончательного ответа следует учесть несколько дополнительных деталей. Более подробную информацию можно найти в статье.\n        "
    },
    {
        "title": "Prompt Chaining",
        "content": "\n            Для повышения надежности и производительности LLM, одним из важных приемов промпт-инжиниринга является разбиение задачи на подзадачи. После того как эти подзадачи определены, для LLM предлогаются подзадачи, а затем ее ответ используется в качестве входных данных для другого запроса. Это то, что называется Создание цепочек промптов, когда задача разбивается на подзадачи с целью создания цепочки операций промптов.\n            Создание цепочек промтов полезно для решения сложных задач, которые LLM может с трудом решить, если будет предложен очень большой промпт. При создание цепочки промптов, цепочки промптов выполняют преобразования или дополнительные процессы над сгенерированными ответами, прежде чем достичь конечного желаемой формы.\n            Помимо повышения производительности, цепочки промптов помогают повысить прозрачность вашего применения LLM, повышает управляемость и надежность. Это означает, что вы можете гораздо проще отлаживать проблемы с ответами модели, а также анализировать и улучшать производительность на различных этапах, которые нуждаются в улучшении.\n            Цепочка промптов особенно полезна при создании диалоговых помощников на базе LLM и улучшении персонализации и пользовательского опыта ваших приложений.\n        "
    },
    {
        "title": "Tree of Thoughts",
        "content": "\n            Для сложных задач, которые требуют исследования или стратегического планирования, традиционные или простые методы создания промптов оказываются недостаточными. Yao et el. (2023) and Long (2023) недавно предложили Tree of Thoughts (ToT), фреймворк, который обобщает метод цепочки мыслей и поощряет исследование мыслей, которые служат промежуточными шагами для общего решения проблем с помощью языковых моделей.\n            ToT поддерживает дерево мыслей, где мысли представляют собой последовательности связной речи, которые служат промежуточными шагами к решению проблемы. Этот подход позволяет лингвистической модели самооценить прогресс промежуточных мыслей в решении проблемы через обдуманный процесс рассуждения. Затем способность лингвистической модели генерировать и оценивать мысли объединяется с алгоритмами поиска (например, поиск в ширину и поиск в глубину), чтобы обеспечить систематическое исследование мыслей с опережением и возвратом назад.\n        "
    },
    {
        "title": "Automatic Prompt Engineer",
        "content": "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the program, optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts."
    },
    {
        "title": "Active-Prompt",
        "content": "\n            Методы цепочки мыслей (CoT) полагаются на фиксированный набор аннотированных человеком образцов. Проблема заключается в том, что эти образцы могут оказаться не наиболее эффективными примерами для различных задач. Чтобы решить эту проблему, Diao и др., (2023) недавно предложили новый подход к формулировке запросов, называемый Active-Prompt, который позволяет адаптировать LLM к различным задачам с использованием специфичных примеров (аннотированных с использованием человеком разработанной цепочки мыслей).\n            Первый шаг заключается в запросе LLM с несколькими примерами CoT или без них. Для набора обучающих вопросов генерируется k возможных ответов. Вычисляется метрика неопределенности на основе этих k ответов (используется показатель расхождения). Наиболее неопределенные вопросы выбираются для аннотации людьми. Затем новые аннотированные образцы используются для вывода каждого вопроса.\n        "
    },
    {
        "title": "Directional Stimulus Prompting",
        "content": "We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts."
    },
    {
        "title": "Program-Aided Language Models",
        "content": "Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (few-shot prompting). Much of this success can be attributed to prompting methods such as chain-of-thought, which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1."
    },
    {
        "title": "ReAct",
        "content": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. "
    },
    {
        "title": "Multimodal CoT",
        "content": "Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. "
    },
    {
        "title": "Graph Prompting",
        "content": "Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks(GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily rely on a large amount of task-specific supervision. To reduce labeling requirement, the pre-train, fine-tune and pre-train, prompt paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-train model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt."
    }
]